# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12pn3OhgWESYkrZH1zR_G6cbChLszqR24
"""

import pandas as pd
import tensorflow as tf
from tensorflow import keras

script = pd.read_csv('/content/Friends_script.csv')
script.head()

print(script.isnull().sum())
script= script.dropna()

from tokenizers import Tokenizer

tokenizer = Tokenizer.from_pretrained("bert-base-cased")
def tokenize(script):
    print(tokenizer.encode(script['Lines']).ids)
tokenize(script.loc[0])

# mapping_token_to_index = {}
# mapping_index_to_token = {}

# def generateVocab(script):
#     vocab = set()

#     for line in script['Lines']:
#         for word in line.split():
#             clean_word = ''.join([char for char in word if char.isalpha()])
#             if clean_word:
#                 vocab.add(clean_word.lower())

#     vocab.update(["<bos>", "<eos>", "<unk>", "<pad>"])

#     vocab = sorted(vocab)

#     # Build mappings
#     for index, token in enumerate(vocab):
#         mapping_token_to_index[token] = index
#         mapping_index_to_token[index] = token

#     return vocab

# def token_to_index(script):
#     tokenized_script = [mapping_token_to_index["<bos>"]]

#     clean_word = ""
#     for char in script:
#         if char == " ":
#             if clean_word:
#                 tokenized_script.append(
#                     mapping_token_to_index.get(clean_word.lower(), mapping_token_to_index["<unk>"])
#                 )
#                 clean_word = ""
#         elif char in [".", "!", "?"]:
#             if clean_word:
#                 tokenized_script.append(
#                     mapping_token_to_index.get(clean_word.lower(), mapping_token_to_index["<unk>"])
#                 )
#                 clean_word = ""
#             tokenized_script.append(mapping_token_to_index["<eos>"])
#         elif char.isalpha():
#             clean_word += char.lower()

#     if clean_word:
#         tokenized_script.append(
#             mapping_token_to_index.get(clean_word.lower(), mapping_token_to_index["<unk>"])
#         )

#     if tokenized_script[-1] != mapping_token_to_index["<eos>"]:
#         tokenized_script.append(mapping_token_to_index["<eos>"])

#     return tokenized_script

# def index_to_token(indexes):
#     return [mapping_index_to_token.get(index, "<unk>") for index in indexes]

vocab = generateVocab(script)




print("Vocabulary size:", len(vocab))
print("First 10 words in vocabulary:", vocab[:10])

text = "Hi I am Ross."
encoded = token_to_index(text)
decoded = index_to_token(encoded)

print(f"Tokens for '{text}':", encoded)
print("Decoded tokens:", decoded)

text = "Hi I am Ross."
encoded = tokenizer.encode(text).ids
decoded = tokenizer.decode(encoded)

print(f"Tokens for '{text}':", encoded)
print("Decoded tokens:", decoded)
type(decoded)

@tf.keras.utils.register_keras_serializable()
class CombinedEmbedding(tf.keras.layers.Layer):
    def __init__(self, vocab_size, num_characters,  max_seq_len, embedding_dim, **kwargs):
        super().__init__(**kwargs)
        self.vocab_size = vocab_size
        self.num_characters = num_characters
        self.max_seq_len = max_seq_len
        self.embedding_dim = embedding_dim

        self.token_embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
        self.speaker_embedding = tf.keras.layers.Embedding(num_characters, embedding_dim)
        self.position_embedding = tf.keras.layers.Embedding(max_seq_len, embedding_dim)

    def call(self, token_ids, speaker_ids):
        seq_len = tf.shape(token_ids)[1]
        batch_size = tf.shape(token_ids)[0]

        token_embed = self.token_embedding(token_ids)

        speaker_embed = self.speaker_embedding(speaker_ids)
        speaker_embed = tf.expand_dims(speaker_embed, axis=1)
        speaker_embed = tf.tile(speaker_embed, [1, seq_len, 1])

        positions = tf.range(start=0, limit=seq_len, delta=1)
        position_embed = self.position_embedding(positions)
        position_embed = tf.expand_dims(position_embed, axis=0)
        position_embed = tf.tile(position_embed, [batch_size, 1, 1])

        return token_embed + speaker_embed + position_embed

    def get_config(self):
        config = super().get_config()
        config.update({
            'vocab_size': self.vocab_size,
            'num_characters': self.num_characters,
            'max_seq_len': self.max_seq_len,
            'embedding_dim': self.embedding_dim,
        })
        return config

unique_speakers = sorted(set(script['Name']))

from tensorflow.keras.preprocessing.sequence import pad_sequences

speaker_to_index = {name: i for i, name in enumerate(unique_speakers)}
tokenized_lines = []
speaker_indices = []
max_seq = 0

for i in range(len(script)):
    line = script['Lines'].iloc[i]
    name = script['Name'].iloc[i]
    line_encode = token_to_index(line)
    tokenized_lines.append(line_encode)
    if name in speaker_to_index:
        speaker_idx = speaker_to_index[name]
        speaker_indices.append(speaker_idx)
    else:
        speaker_indices.append(0)
    max_seq = max(len(line_encode), max_seq)

token_ids_padded = pad_sequences(tokenized_lines, maxlen=max_seq, padding='post')

token_ids_tensor = tf.constant(token_ids_padded, dtype=tf.int32)
speaker_ids_tensor = tf.constant(speaker_indices, dtype=tf.int32)

max_seq

def create_padding_mask(seq):
    # Create a mask for the padding values (assuming 0 is your padding index)
    seq = tf.cast(tf.math.equal(seq, mapping_token_to_index["<pad>"]), tf.float32) # Use "<pad>" index
    # Add extra dimensions to add the padding to the attention logits.
    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)

tf.shape(token_ids_tensor)
tf.shape(speaker_ids_tensor)

emb = CombinedEmbedding(len(vocab),len(unique_speakers),246,256)
# output = emb(token_ids_tensor, speaker_ids_tensor)

@tf.keras.utils.register_keras_serializable()
class DecoderBlock(tf.keras.layers.Layer):
    def __init__(self, MHA_num_heads, emb_dim, ff_dim, dropout_rate=0.1, **kwargs):
        super().__init__(**kwargs)
        self.MHA_num_heads = MHA_num_heads
        self.emb_dim = emb_dim
        self.ff_dim = ff_dim
        self.dropout_rate = dropout_rate

        self.MHA = tf.keras.layers.MultiHeadAttention(
            num_heads=MHA_num_heads,
            key_dim=emb_dim // MHA_num_heads,
            output_shape=emb_dim
        )
        self.ff = tf.keras.Sequential([
            tf.keras.layers.Dense(ff_dim, activation='relu'),
            tf.keras.layers.Dense(emb_dim)
        ])
        self.layernom1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernom2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)
        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)

    def call(self, x, training, attention_mask=None):
        tf.debugging.assert_shapes([(x, ['batch', 'seq', 'emb_dim'])], message="Input shape to DecoderBlock is incorrect")

        attention_output = self.MHA(query=x, value=x, key=x, attention_mask=attention_mask, training=training)
        attention_output = self.dropout1(attention_output, training=training)
        out1 = self.layernom1(x + attention_output)

        ff_output = self.ff(out1)
        ff_output = self.dropout2(ff_output, training=training)
        out2 = self.layernom2(out1 + ff_output)

        return out2

    def get_config(self):
        config = super().get_config()
        config.update({
            'MHA_num_heads': self.MHA_num_heads,
            'emb_dim': self.emb_dim,
            'ff_dim': self.ff_dim,
            'dropout_rate': self.dropout_rate,
        })
        return config


@tf.keras.utils.register_keras_serializable()
class DecoderTransformer(tf.keras.Model):
    def __init__(self, vocab_size, num_characters, max_seq, embedding_dim, MHA_num_heads, ff_dim, num_decoder_blocks, dropout_rate=0.1, **kwargs):
        super().__init__(**kwargs)
        self.vocab_size = vocab_size
        self.num_characters = num_characters
        self.max_seq = max_seq
        self.embedding_dim = embedding_dim
        self.MHA_num_heads = MHA_num_heads
        self.ff_dim = ff_dim
        self.num_decoder_blocks = num_decoder_blocks
        self.dropout_rate = dropout_rate

        self.embedding = CombinedEmbedding(vocab_size, num_characters, max_seq, embedding_dim)
        self.decoder_blocks = [DecoderBlock(MHA_num_heads, embedding_dim, ff_dim, dropout_rate) for _ in range(num_decoder_blocks)]
        self.final_dense = tf.keras.layers.Dense(vocab_size)

    def call(self, token_ids_tensor, speaker_ids_tensor, attention_mask=None, training=None):
        x = self.embedding(token_ids_tensor, speaker_ids_tensor)
        tf.debugging.assert_shapes([(x, ['batch', 'seq', 'emb_dim'])], message="Shape after embedding is incorrect")

        for i, decoder_block in enumerate(self.decoder_blocks):
            x = decoder_block(x, training=training, attention_mask=attention_mask)
            tf.debugging.assert_shapes([(x, ['batch', 'seq', 'emb_dim'])], message=f"Shape after decoder block {i} is incorrect")

        logits = self.final_dense(x)
        return logits

    def get_config(self):
        config = super().get_config()
        config.update({
            'vocab_size': self.vocab_size,
            'num_characters': self.num_characters,
            'max_seq': self.max_seq,
            'embedding_dim': self.embedding_dim,
            'MHA_num_heads': self.MHA_num_heads,
            'ff_dim': self.ff_dim,
            'num_decoder_blocks': self.num_decoder_blocks,
            'dropout_rate': self.dropout_rate,
        })
        return config

from tensorflow.keras.optimizers import Adam
batch_size = 16
dataset = tf.data.Dataset.from_tensor_slices((token_ids_tensor, speaker_ids_tensor))
dataset = dataset.shuffle(buffer_size=len(script))
batched_dataset = dataset.batch(batch_size)
optimizer = Adam(learning_rate=0.0001)

model = DecoderTransformer(
    vocab_size=len(vocab),
    num_characters=len(unique_speakers),
    max_seq=max_seq,
    embedding_dim=128,
    MHA_num_heads=8,
    ff_dim=1024,
    num_decoder_blocks=3,
    dropout_rate=0.1
)

num_epochs = 8

for epoch in range(num_epochs):
    print(f"Epoch {epoch+1}/{num_epochs}")
    total_loss_epoch = []
    for step, (batch_token_ids, batch_speaker_ids) in enumerate(batched_dataset):
        with tf.GradientTape() as tape:
            input_batch_token_ids = batch_token_ids[:,:-1]
            output_batch_token_ids = batch_token_ids[:,1:]
            padding_mask = create_padding_mask(input_batch_token_ids)

            predicted_token = model(
                input_batch_token_ids,
                batch_speaker_ids,
                attention_mask=padding_mask,
                training=True
            )
            cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
            loss = cross_entropy(output_batch_token_ids, predicted_token)
        gradients = tape.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(zip(gradients, model.trainable_variables))
        total_loss_epoch.append(loss.numpy())
        if step % 100 == 0:
             print(f"Epoch {epoch+1}, Step {step}, Loss: {loss.numpy()}")
    avg_loss = sum(total_loss_epoch)/len(total_loss_epoch) if total_loss_epoch else 0
    print(f"Epoch {epoch+1} Average Loss: {avg_loss}")

model.save('decoder_transformer_model.keras')

import numpy as np
def generate(start_string, character_name, max_generate_length, temperature=0.7):

    if character_name not in speaker_to_index:
        print(f"Speaker '{character_name}' not found. Using default speaker index 0.")
        speaker_idx = 0
    else:
        speaker_idx = speaker_to_index[character_name]

    speaker_id_tensor = tf.constant([speaker_idx], dtype=tf.int32)
    input_tokens = token_to_index(start_string)

    generated_sequence_ids = list(input_tokens)
    if generated_sequence_ids and generated_sequence_ids[-1] == mapping_token_to_index["<eos>"]:
        generated_sequence_ids.pop()

    full_output_ids = list(generated_sequence_ids)

    for _ in range(max_generate_length):
        current_input_ids = list(full_output_ids)
        if len(current_input_ids) >= max_seq:
            current_input_ids = current_input_ids[-(max_seq - 1):]

        current_tokens_tensor = tf.constant([current_input_ids], dtype=tf.int32)

        padding_mask = create_padding_mask(current_tokens_tensor)

        predictions = model(
            current_tokens_tensor,
            speaker_id_tensor,
            attention_mask=padding_mask,
            training=False
        )

        predicted_logits_for_next_token = predictions[:, -1, :]

        scaled_logits = predicted_logits_for_next_token / temperature

        predicted_id = tf.random.categorical(scaled_logits, num_samples=1).numpy()[0][0]

        full_output_ids.append(predicted_id)

        # if predicted_id == mapping_token_to_index["<eos>"]:
        #     break # Stop if <eos> is generated

    decoded_tokens_list = index_to_token(full_output_ids)

    if decoded_tokens_list and decoded_tokens_list[0] == "<bos>":
        decoded_tokens_list = decoded_tokens_list[1:]

    final_sentence = " ".join(decoded_tokens_list)
    final_sentence = final_sentence.replace(" <eos>", ".").replace("<eos>", ".") # Handle EOS at the very end
    final_sentence = final_sentence.replace(" <unk>", " [UNK]").replace("<unk>", "[UNK]")
    final_sentence = final_sentence.replace(" <pad>", "").replace("<pad>", "") # Remove any pad tokens

    return final_sentence.strip()

speaker = "Joey"
generated_line = generate("why are", speaker, max_generate_length=350)
print(f"{speaker}: {generated_line}")